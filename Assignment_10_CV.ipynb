{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why don't we start all of the weights with zeros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->If all the weights are initialized to zeros, the derivatives will remain same for every w in W[l]. As a result, neurons will learn same features in each iterations. This problem is known as network failing to break symmetry. And not only zero, any constant initialization will produce a poor result. It leads the neurons to learn the same features during training. In fact, any constant initialization scheme will perform very poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why is it beneficial to start weights with a mean zero distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Restricting the weights to have mean at 0 and std at 1 can make the weights as small as possible, which make it convenient for regularization. Normal distribution has the maximum entropy given the mean and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is dilated convolution, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Dilated Convolutions are a type of convolution that “inflate” the kernel by inserting holes between the kernel elements. An additional parameter (dilation rate) indicates how much the kernel is widened. There are usually spaces inserted between kernel elements.\n",
    "It is a technique that expands the kernel (input) by inserting holes between its consecutive elements. In simpler terms, it is the same as convolution but it involves pixel skipping, so as to cover a larger area of the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is TRANSPOSED CONVOLUTION, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Transposed convolutions are standard convolutions but with a modified input feature map. The stride and padding do not correspond to the number of zeros added around the image and the amount of shift in the kernel when sliding it across the input, as they would in a standard convolution operation.\n",
    "Transposed Convolutions is a revolutionary concept for applications like image segmentation, super-resolution etc but sometimes it becomes a little trickier to understand.\n",
    "Transposed Convolutions are used to upsample the input feature map to a desired output feature map using some learnable parameters.\n",
    "The basic operation that goes in a transposed convolution is explained below:\n",
    "1. Consider a 2x2 encoded feature map which needs to be upsampled to a 3x3 feature map.\n",
    "2. We take a kernel of size 2x2 with unit stride and zero padding.\n",
    "3. Now we take the upper left element of the input feature map and multiply it with every element of the kernel \n",
    "5. Now some of the elements of the resulting upsampled feature maps are over-lapping. To solve this issue, we simply add the elements of the over-lapping positions.\n",
    "6. The resulting output will be the final upsampled feature map having the required spatial dimensions of 3x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Explain Separable convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->A Separable Convolution is a process in which a single convolution can be divided into two or more convolutions to produce the same output. A single process is divided into two or more sub-processes to achieve the same effect.\n",
    "There are two main types of separable convolutions: spatial separable convolutions, and depthwise separable convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.What is depthwise convolution, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Depthwise Convolution is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D convolution performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output.\n",
    "If there is no bias term, ordinary convolution has K×C×R×S K × C × R × S parameters, whereas depthwise separable convolution has C×R×S+K×C C × R × S + K × C parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Capsule networks are what they sound like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->In a capsule network, each capsule is made up of a group of neurons with each neuron's output representing a different property of the same feature. This provides the advantage of recognizing the whole entity by first recognizing its parts. The input to a capsule is the output (or features) from a CNN.\n",
    "a traditional neuron in a neural net performs the following scalar operations:\n",
    "\n",
    "Weighting of inputs\n",
    "Sum of weighted inputs\n",
    "Nonlinearity\n",
    "These operations are slightly changed within capsules and are performed as follows:\n",
    "\n",
    "Matrix multiplication of input vectors with weight matrices. This encodes really important spatial relationships between low-level features and high-level features within the image.\n",
    "Weighting input vectors. These weights decide which higher level capsule the current capsule will send it’s output to. This is done through a process of dynamic routing, which I’ll talk more about soon.\n",
    "Sum of weighted input vectors. (Nothing special about this)\n",
    "Nonlinearity using “squash” function. This function takes a vector and “squashes” it to have a maximum length of 1, and a minimum length of 0 while retaining its direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Why is POOLING such an important operation in CNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Pooling layers are useful when you want to detect an object in an image regardless of its position in the image. The consequence of adding pooling layers is the reduction of overfitting, increased efficiency, and faster training times in a CNN model.It is used to reduce the size of feature maps, which in turn makes computation faster because the number of training parameters is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.What are receptive field and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The receptive field encompasses the sensory receptors that feed into sensory neurons and thus includes specific receptors on a neuron as well as collectives of receptors that are capable of activating a neuron via synaptic connections.\n",
    "Receptive field is the receptor area which when stimulated results in a response of a particular sensory neuron."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
